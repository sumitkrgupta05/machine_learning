{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c3b213-2b56-47d0-8ab4-be0e1ae10eed",
   "metadata": {},
   "source": [
    "# Q) Define ML with real life example??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69432c5a-9a02-4bbf-b312-26185803a203",
   "metadata": {},
   "source": [
    "i) machine learning is set of techniques to make computers better at doing things that humans (traditionally) can do better then machines.\n",
    "\n",
    "ii) ml involves making machines learn things like humans do.\n",
    "\n",
    "iii) ml mainly consists of 2 parts suchs as:\n",
    "   1> Deep Learning\n",
    "   2> Mathematical Models\n",
    "\n",
    "iv) Real life ml use:\n",
    "-- in mail we see that theres a spam section. {which automatically detect the mail and put the bkr mail in spam}.\n",
    "\n",
    "--personal assistance such as alexa which understand the human lang and do their task.\n",
    "\n",
    "--youtube recommendation is another good example of ml.\n",
    "\n",
    "--driverless car such as tesla is a very good example of ml."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92500e-e147-48b4-bb4d-f566ad954492",
   "metadata": {},
   "source": [
    "# Q) Define Linear Regression??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16d2cd7a-c83e-4451-adcb-c964b1871719",
   "metadata": {},
   "source": [
    "=> Linear regression is an algorithm that provides a linear relationship between an independent variable and a dependent variable to predict the outcome of future events. It is a statistical method used in data science and machine learning for predictive analysis.\n",
    "\n",
    "=> Thus, linear regression is a supervised learning algorithm that simulates a mathematical relationship between variables and makes predictions for continuous or numeric variables such as sales, salary, age, product price, etc.\n",
    "\n",
    "=> A sloped straight line represents the linear regression model.\n",
    "\n",
    "=> Best Fit Line for a Linear Regression Model\n",
    "\n",
    "X-axis = Independent variable\n",
    "\n",
    "Y-axis = Output / dependent variable\n",
    "\n",
    "Line of regression = Best fit line for a model\n",
    "\n",
    "Here, a line is plotted for the given data points that suitably fit all the issues. Hence, it is called the ‘best fit line.’ The goal of the linear regression algorithm is to find this best fit line seen in the above figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e867b-72b2-42ca-9444-44ef14937124",
   "metadata": {},
   "source": [
    "# Q) Multiple Linear regression."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6ddf82e-f5d3-46d6-9809-cddb90059105",
   "metadata": {},
   "source": [
    "Multiple Linear Regression: This is the more common scenario you might be referring to as \"linear regression multivariation\". It's an extension of simple linear regression (one dependent variable and one independent variable) to cases where you have one dependent variable and multiple independent variables. You're essentially trying to find a linear relationship between a single outcome and several factors that might influence it.\n",
    "\n",
    "For example, you might be studying house prices. In simple linear regression, you might look at just square footage to predict price. But with multiple linear regression, you could consider factors like number of bedrooms, location, and lot size to create a more comprehensive model.\n",
    "\n",
    "Here are some key things to remember about multiple linear regression:\n",
    "\n",
    "* It's a powerful tool for understanding how multiple factors influence a single outcome.\n",
    "* Choosing the right independent variables is crucial for an accurate model.\n",
    "* There are statistical tests to assess the model's fit and identify potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c5254-cfab-40b5-a00d-655d08cc3a38",
   "metadata": {},
   "source": [
    "# Q) Multivariate Regression "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d44be915-bd10-4185-b90e-c17c251afd89",
   "metadata": {},
   "source": [
    "Multivariate Regression: This flips the script and focuses on multiple dependent variables. You still have multiple independent variables, but you're trying to predict the values of several dependent variables simultaneously.\n",
    "\n",
    "For instance, imagine you're analyzing student performance. You might have independent variables like study habits and class attendance, but you want to predict both their scores on exams and their final grades (two dependent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13df0d5-d0b0-48a5-af8c-b9ef007f3b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "653a5842-1a17-482b-bc68-68d49874d14f",
   "metadata": {},
   "source": [
    "Gradient descent and cost function are two essential concepts that work together in machine learning, especially for training models. Here's a breakdown of each:\n",
    "\n",
    "Cost Function:\n",
    "\n",
    "* Imagine you're training a model to predict something, like house prices. The cost function essentially measures how well your model is performing at a particular point in time.\n",
    "* It quantifies the error between the predictions made by your model and the actual values you're trying to predict.\n",
    "* There are different cost functions used for different tasks. For linear regression, a common cost function is the Mean Squared Error (MSE), which calculates the average squared difference between predicted and actual values.\n",
    "* The goal in machine learning is to minimize the cost function, leading to a model with the least amount of error.\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "* This is an optimization algorithm that helps achieve the goal of minimizing the cost function.\n",
    "* Think of it like rolling a ball down a bumpy hill. You want the ball (your model) to end up at the lowest point (minimum cost function).\n",
    "* Gradient descent works iteratively. Here's the gist of the process:\n",
    "    It calculates the gradient of the cost function. The gradient tells you the direction of the steepest descent (where the error increases the fastest).\n",
    "    It then takes a small step in the opposite direction of the gradient. This means moving the model's parameters (like weights and biases) in a way   that should reduce the cost function.\n",
    "    It repeats steps 1 and 2 until the cost function converges to a minimum value, signifying the model has learned well from the data.\n",
    "    Together:\n",
    "\n",
    "The cost function provides feedback on how well the model is doing, while gradient descent uses that feedback to adjust the model's parameters and improve its performance.\n",
    "By iteratively minimizing the cost function, gradient descent helps the model learn the optimal parameters that can make accurate predictions.\n",
    "Here's an analogy: Imagine you're training a dog to fetch a ball. The distance between where the dog drops the ball and your hand is the cost function (error). By giving the dog a treat (positive reinforcement) when it gets closer (reduces the error), you're essentially using a form of gradient descent to train the dog to fetch the ball directly into your hand (minimize the cost function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cfc57-06a6-4b2b-ab40-adf8abd536fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
